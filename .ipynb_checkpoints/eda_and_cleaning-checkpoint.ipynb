{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into geopandas dataframe\n",
    "crime = gpd.read_file(\"https://opendata.arcgis.com/datasets/716338a41410457bb415a4bae2b2ad3e_0.geojson\", low_memory=False)\n",
    "\n",
    "# confirm load\n",
    "crime.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dataframe\n",
    "shape = crime.shape\n",
    "info = crime.info()\n",
    "head = crime.head()\n",
    "\n",
    "print(shape)\n",
    "print(info)\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need\n",
    "cols2drop = ['Index_', 'Division', 'reporteddate', 'location_type', 'ucr_code', 'ucr_ext', 'offence', 'reportedyear', 'reportedmonth', \n",
    "             'reportedday', 'reporteddayofyear', 'reporteddayofweek', 'reportedhour', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a quick peek at crime by year\n",
    "crime['occurrenceyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there appears to be a big change in crime data between 2013 and 2014 - a jump by 30 thousand a year!\n",
    "# data from 2014 onwards appears to be similar and represents over 200,000 of the 281,692 entries in this dataset\n",
    "# it is not clear why this is the case - perhaps due to a change in tracking or recording practices\n",
    "# in order to ensure our analysis is not skewed, we will delete all occurences from 2013 or earlier\n",
    "crime = crime[crime['occurrenceyear'] > 2013]\n",
    "\n",
    "# confirm reduced dataframe\n",
    "crime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take another look at crime by year\n",
    "crime['occurrenceyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reduced dataframe\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at crimes by dates\n",
    "dates = crime[['occurrencedate', 'occurrenceyear', 'occurrencemonth', 'occurrenceday']]\n",
    "dates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save reduced data locally\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'crimeData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "crime.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data from local\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'crimeData.csv.gz'\n",
    "crime = gpd.read_csv(os.path.join(path,fn), compression='gzip', low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "crime.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "crime.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no null values - great!\n",
    "# now count nans by columns\n",
    "crime.isna().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nans by rows\n",
    "crime.isna().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no nans - great!\n",
    "# documentation indicates that for locations identified outside the city of Toronto limits or as invalidated locations\n",
    "# the division/neighbourhood designation will be ‘NSA’ to indicate ‘Not Specified Area.’\n",
    "# we will identify and delete these as we are only concerned with occurences with identifiable locations\n",
    "crime.loc[crime['Hood_ID'] == 'NSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that when 'Hood_ID' == NSA, 'Neighbourhood' is also NSA, let's check to see if we get the same results\n",
    "crime.loc[crime['Neighbourhood'] == 'NSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they match! we will delete these\n",
    "crime = crime[crime.Hood_ID != 'NSA']\n",
    "crime = crime[crime.Neighbourhood != 'NSA']\n",
    "\n",
    "# confirm reduced dataframe\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm reduction meets amount in NSA query\n",
    "x = 281692-277071\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "\n",
    "# 1. crime types - 2 fields\n",
    "    # offence and MCI fields - from documentation, we know that offence is a non-standardized open field, whereas MCI is categorized\n",
    "    # we will drop offence and keep MCI\n",
    "    \n",
    "# 2. neighbourhood - 2 fields\n",
    "    # Hood_ID and Neighbourhood fields are duplicates, but should bothe be left in for fleixbility in matching with neighoburhood polygons later\n",
    "    # we will check unique values to ensure both have the same amount and keep both of these \n",
    "    \n",
    "# 3. cateogrical fields - 4 fields\n",
    "    # location_type, premises_type, MCI, and Neighbourhood fields are all categorical data as indicated in the documentation\n",
    "    # location_type is too detailed for our purposes\n",
    "    # we will delete location_type and convert the rest to categories - NOTE GPKG WOULD NOT ALLOW CATEGORIES\n",
    "    \n",
    "# 4. ids - 2 fields\n",
    "    # event_unique_id and ObjectId fields seem to both be unique values, but they are different data types\n",
    "    # documentation suggests that occurences with multiple types of crimes will show up as multiple entries\n",
    "    # therefore, one of these fields will have multiple entries and the other will have unique values - we will need to confirm differences, address them, and possibly convert to int\n",
    "\n",
    "# 5. dates - 7 fields\n",
    "    # occurrencedate, occurrenceyear, occurrencemonth, occurrenceday, occurrencedayofyear, occurrencedayofweek, and occurrencehour fields\n",
    "    # occurencedate is generalized and is sufficient for our purposes, we will delete the rest\n",
    "    \n",
    "# 6. location - 3 fields\n",
    "    # we will keep geometry\n",
    "    # we will check lat and long to see if there are outliers to remove, then we will delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - drop offence field\n",
    "cols2drop2 = ['location_type','offence']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop2:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - count unique values of neighbourhood fields\n",
    "count1 = crime.Hood_ID.nunique()\n",
    "count2 = crime.Neighbourhood.nunique()\n",
    "print(count1)\n",
    "print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they match and we will keep both\n",
    "\n",
    "# NOTE gpkg will not accept categories so the rest of this cell has been hashed out\n",
    "\n",
    "# 3 - convert location_type, premises_type, MCI, and neighbourhood fields to cateogrical data\n",
    "#for c in ['premises_type', 'MCI', 'Neighbourhood']:\n",
    "#    crime[c] = crime[c].astype('category')\n",
    "    \n",
    "# confirm conversion to categories\n",
    "#crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also convert Hood_ID to int\n",
    "for c in ['Hood_ID']:\n",
    "    crime[c] = crime[c].astype('int')\n",
    "\n",
    "# confirm conversion to int\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - we have 277071 rows, let's check count of unique values of id fields compared to rows\n",
    "count3 = crime.event_unique_id.nunique()\n",
    "count4 = crime.ObjectId.nunique()\n",
    "print(count3)\n",
    "print(count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the event_unique_id field has duplicate values, whereas the ObjectID field has a unique value for every row\n",
    "# as per documentation one event can have mutliple crime types - for example, a single event could have both an assault and a theft\n",
    "# for our purposes, we want to ensure we count each crime type seperately\n",
    "# we will delete the event_unique_id field and keep the ObjectId field\n",
    "cols2drop3 = ['event_unique_id']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop3:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - check dates fields\n",
    "dates = crime[['occurrencedate', 'occurrenceyear', 'occurrencemonth', 'occurrenceday', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']]\n",
    "dates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our purposes occurrencedate is all that is required and the rest can be dropped\n",
    "cols2drop4 = ['occurrenceyear', 'occurrencemonth', 'occurrenceday', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop4:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['occurrencedate'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - we will check lat and long fields for outliers\n",
    "location = crime[['Long', 'Lat']]\n",
    "location.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min in the lat field is showing as 0 which is definitely not in Toronto\n",
    "\n",
    "# let's check how many values are equal to 0\n",
    "print((crime['Lat'] == 0).sum())\n",
    "\n",
    "# vs. all values\n",
    "print((crime['Lat']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete these values\n",
    "crime = crime[crime.Lat != 0]\n",
    "\n",
    "# check values again\n",
    "location2 = crime[['Long', 'Lat']]\n",
    "location2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that fixed the problem\n",
    "# we can now delete Long and Lat columns\n",
    "cols2drop5 = ['Long', 'Lat']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop5:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have another look at the data\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 278 rows have been removed\n",
    "# another look\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are completing an analysis of the impact of COVID-19 lockdowns on crime, we will add an additonal column for lockdown information\n",
    "# from research we know that Toronto had three lockdowns as follows:\n",
    "\n",
    "lockdown = {'ONE':['2020-03-23','2020-07-31'],\n",
    "            'TWO':['2020-11-23','2021-03-08'],\n",
    "            'THREE':['2021-04-08','2021-06-02']}\n",
    "\n",
    "crime['occurrencedate'] = pd.to_datetime(crime.occurrencedate.values, infer_datetime_format=True)\n",
    "\n",
    "for k, (s,e) in lockdown.items():\n",
    "    crime.loc[crime['occurrencedate'].between(s,e), 'lockdownNum'] = k\n",
    "\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in lockdownNum field with NONE (no lockdown)\n",
    "crime['lockdownNum'] = crime['lockdownNum'].fillna('NONE')\n",
    "\n",
    "# check revised number of occurrences by lockdownNum\n",
    "crime.lockdownNum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE gpkg will not accept categories so this cell has been hashed out\n",
    "\n",
    "# convert lockdownNum field to categorical data\n",
    "#for c in ['lockdownNum']:\n",
    "#    crime[c] = crime[c].astype('category')\n",
    "\n",
    "# confirm changes\n",
    "#crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary lockdown column based on lockdownNum column where 1 = in lockdown, and 0 = not in lockdown\n",
    "crime['lockdownBinary'] = ['1' if x == 'ONE' else '1' if x == 'TWO' else '1' if x == 'THREE' else '0' for x in crime['lockdownNum']]\n",
    "\n",
    "#confirm new column\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lockdownBinary to int\n",
    "for c in ['lockdownBinary']:\n",
    "    crime[c] = crime[c].astype('int')\n",
    "    \n",
    "# confirm conversion to int\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### crime.sort_values(by=['occurrenceyear', 'occurrencemonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random check of data frame\n",
    "crime.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at data\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save crime as clean data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'crimeData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "crime.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save crime as geodataframe\n",
    "\n",
    "# Set save location\n",
    "path = os.path.join('data','geo')\n",
    "fn = 'crimeData.gpkg'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "\n",
    "crime.to_file(os.path.join(path,fn), index=False, driver='GPKG')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at the Covid case data\n",
    "# open covid-19 case data into a pandas dataframe\n",
    "\n",
    "url = 'https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/64b54586-6180-4485-83eb-81e8fae3b8fe/resource/fff4ee65-3527-43be-9a8a-cb9401377dbc/download/COVID19%20cases.csv'\n",
    "covidcases = pd.read_csv(url, low_memory=False)\n",
    "\n",
    "# confirm load\n",
    "print(f\"Data frame is {covidcases.shape[0]:,} rows x {covidcases.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "covidcases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(covidcases.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data\n",
    "covidcases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need\n",
    "cols2drop6 = ['Outbreak Associated', 'Age Group', 'Source of Infection', 'Client Gender', 'Outcome', 'Currently Hospitalized', \n",
    "              'Currently in ICU', 'Currently Intubated', 'Ever Hospitalized', 'Ever in ICU', 'Ever Intubated']\n",
    "for col in df.columns:\n",
    "    if col in cols2drop6:\n",
    "        df.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rough covid case data\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'covidData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "covidcases.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload rough covid case data\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'covidData.csv.gz'\n",
    "df = pd.read_csv(os.path.join(path,fn), compression='gzip', low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(df.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "df.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "df.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many null values\n",
    "# let's look at remaining columns in more detail to see if we can drop more\n",
    "df.head()[['_id', 'Assigned_ID', 'Neighbourhood Name', 'FSA', 'Classification', 'Episode Date', 'Reported Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "# 1. ids - 2 fields\n",
    "    # the _id and Assigned_ID fields seem to be identical and have the same amount\n",
    "    # however, documentation suggests Assigned_ID is from Toronto Public Health, not the database, and cases can disappear\n",
    "    # based on this, we will delete Assigned_ID\n",
    "\n",
    "# 2. Locations - 2 fields\n",
    "    # the Neighbourhood Name and Forward Sortation Area (FSA) fields both can be used to geolocate the cases\n",
    "    # documentation suggests that the FSA field and Census Tracts (CTs) were used to determine the Neighbourhood Name\n",
    "    # documentation also mentions that neighbourhood information is missing for cases with missing postalcodes\n",
    "    # for our purposes aggregate numbers are okay and we don't need to geolocate so we will drop both of these fields \n",
    "\n",
    "# 3. Status - 1 field\n",
    "    # According to the documentation, the Classification field classifies cases as either confirmed or probable\n",
    "    # as with above, for our purposes, aggregate numbers are okay, we will delete probable cases and keep confirmed\n",
    "    \n",
    "# 4. dates - 2 fields\n",
    "    # we do not need both of these dates, so we will keep the reported date as an official record and drop the episode date\n",
    "    # change reported date to reported_date for ease\n",
    "    \n",
    "# drop columns we don't need\n",
    "cols2drop7 = ['Assigned_ID', 'Neighbourhood Name', 'FSA', 'Episode Date']\n",
    "for col in df.columns:\n",
    "    if col in cols2drop7:\n",
    "        df.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "df.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "df.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more nulls - perfect!\n",
    "# check classification counts \n",
    "df.Classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove PROBABLE cases, keep CONFIRMED cases\n",
    "df = df[df.Classification != 'PROBABLE']\n",
    "\n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts\n",
    "df.Classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename reported date field\n",
    "df.rename(columns = {'Reported Date':'ReportedDate'}, inplace = True)\n",
    "\n",
    "# confirm renamed column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last thing we want to do is to get a summary count of all confirmed cases per day\n",
    "casesperday = df.groupby(['ReportedDate']).size().reset_index(name='CasesPerDay')\n",
    "casesperday = casesperday.sort_values(by=['CasesPerDay'], ascending=False)\n",
    "casesperday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned covid case data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'covidData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "casesperday.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Key Dates Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at key dates\n",
    "# load key dates data into a pandas dataframe\n",
    "\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'keyDates.csv'\n",
    "kd = pd.read_csv(os.path.join(path,fn), low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "kd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "kd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "\n",
    "# 1. dates - 3 fields\n",
    "    # Date field - refers to date of event (see keydates) - will leave as object\n",
    "    # LockdownDates - is binary - refers to the dates the lockdowns began and ended - will be used for visualization - will leave as int\n",
    "    # KeyDates - is binary - refers to key dates for headlines in the visualization - will leave as int\n",
    "    \n",
    "# 2. Event - 1 field\n",
    "    # can be used as headlines or scrolling tickers for the visualization\n",
    "    \n",
    "# 3. cateogrical fields - 1 fields\n",
    "    # Lockdown field - represents which lockdown\n",
    "    # we will convert these to categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Key Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - convert Lockdown field to cateogrical data\n",
    "for c in ['Lockdown']:\n",
    "    kd[c] = kd[c].astype('category')\n",
    "    \n",
    "# confirm conversion to categories\n",
    "kd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Key Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned key dates data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'keyDates.csv'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "kd.to_csv(os.path.join(path,fn), index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbourhood Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at the neighbourhood boundaries\n",
    "# read in json file\n",
    "tor_nbs = gpd.read_file('https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/4def3f65-2a65-4a4f-83c4-b2a4aed72d46/resource/9ce32bd1-91ac-4422-925a-bdc256702756/download/Neighbourhoods%20-%20historical%20140.geojson')\n",
    "\n",
    "# confirm load\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "tor_nbs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(tor_nbs.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need - we only need to be able to match neighbourhood geography to Hood_ID and Neighbourhood columns within the crimeData table\n",
    "cols2drop8 = ['AREA_ID', 'AREA_ATTR_ID', 'PARENT_AREA_ID', 'AREA_DESC', 'CLASSIFICATION', 'CLASSIFICATION_CODE', 'OBJECTID']\n",
    "for col in tor_nbs.columns:\n",
    "    if col in cols2drop8:\n",
    "        tor_nbs.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "tor_nbs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data review\n",
    "\n",
    "# 1 - area name fields\n",
    "# we will rename this column to Neighbourhood to match our data\n",
    "\n",
    "# 2 - area code fields\n",
    "# we need to confirm which of short_code or long_code is more accurate, then delete the other\n",
    "# we will then rename the column to Hood_ID to match our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - rename area name column to Neighbourhood to match our data\n",
    "tor_nbs.rename(columns={'AREA_NAME': 'Neighbourhood'}, inplace=True)\n",
    "\n",
    "# check to confirm\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - area code fields\n",
    "# we need to confirm which of short_code or long_code is more accurate, then delete the other\n",
    "# we will then rename the column to Hood_ID to match our data\n",
    "count5 = tor_nbs.AREA_SHORT_CODE.nunique()\n",
    "count6 = tor_nbs.AREA_LONG_CODE.nunique()\n",
    "print(count5)\n",
    "print(count6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they seem to be identical, let's do one more check\n",
    "tor_nbs.AREA_SHORT_CODE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and...\n",
    "tor_nbs.AREA_LONG_CODE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmed, they are identifcal, we will delete AREA_LONG_CODE...\n",
    "cols2drop9 = ['AREA_LONG_CODE']\n",
    "for col in tor_nbs.columns:\n",
    "    if col in cols2drop9:\n",
    "        tor_nbs.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# .. and change the name of AREA_SHORT_CODE to Hood_ID\n",
    "tor_nbs.rename(columns={'AREA_SHORT_CODE': 'Hood_ID'}, inplace=True)\n",
    "        \n",
    "# confirm changes\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check plot \n",
    "print(tor_nbs.geometry.crs)\n",
    "print(tor_nbs.total_bounds)\n",
    "ax = tor_nbs.plot(figsize=(18,14), \n",
    "                  edgecolor='red', \n",
    "                  facecolor='none', \n",
    "                  linewidth=1, \n",
    "                  alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see how things look\n",
    "\n",
    "# plot crime\n",
    "print(crime.geometry.crs)\n",
    "print(crime.total_bounds)\n",
    "crime.plot(figsize=(18,14), marker='*', color='green', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add toronto neighbourhoods\n",
    "base = tor_nbs.plot(figsize=(18,14), color='white', edgecolor='black')\n",
    "\n",
    "# add crime locations\n",
    "crime.plot(ax=base, figsize=(18,14), marker='o', color='red', markersize=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks good!\n",
    "# NOTE - it appears that some dots along the edges are outside toronto\n",
    "    # HOWEVER - documentation notes crime locations moved to closest intersection to maintain annonimity\n",
    "    # AS A RESULT - All included points have been verified as within an identified Toronto Neighbourhood, but may not appear so visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tor_nbs as a geodataframe\n",
    "\n",
    "# Set save location\n",
    "path = os.path.join('data','geo')\n",
    "fn = 'tor_nbs.gpkg'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "print(f\"Using '{fn}' as basis for saving data...\")\n",
    "\n",
    "tor_nbs.to_file(os.path.join(path,fn), driver='GPKG')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
