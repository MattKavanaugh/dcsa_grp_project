{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into geopandas dataframe\n",
    "crime = gpd.read_file(\"https://opendata.arcgis.com/datasets/716338a41410457bb415a4bae2b2ad3e_0.geojson\", low_memory=False)\n",
    "\n",
    "# confirm load\n",
    "crime.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index_', 'event_unique_id', 'Division', 'occurrencedate', 'reporteddate', 'location_type', 'premises_type', 'ucr_code', 'ucr_ext', 'offence', 'reportedyear', 'reportedmonth', 'reportedday', 'reporteddayofyear', 'reporteddayofweek', 'reportedhour', 'occurrenceyear', 'occurrencemonth', 'occurrenceday', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour', 'MCI', 'Hood_ID', 'Neighbourhood', 'Long', 'Lat', 'ObjectId', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281692, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at dataframe\n",
    "crime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 281692 entries, 0 to 281691\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count   Dtype   \n",
      "---  ------               --------------   -----   \n",
      " 0   Index_               281692 non-null  int64   \n",
      " 1   event_unique_id      281692 non-null  object  \n",
      " 2   Division             281692 non-null  object  \n",
      " 3   occurrencedate       281692 non-null  object  \n",
      " 4   reporteddate         281692 non-null  object  \n",
      " 5   location_type        281692 non-null  object  \n",
      " 6   premises_type        281692 non-null  object  \n",
      " 7   ucr_code             281692 non-null  int64   \n",
      " 8   ucr_ext              281692 non-null  int64   \n",
      " 9   offence              281692 non-null  object  \n",
      " 10  reportedyear         281692 non-null  int64   \n",
      " 11  reportedmonth        281692 non-null  object  \n",
      " 12  reportedday          281692 non-null  int64   \n",
      " 13  reporteddayofyear    281692 non-null  int64   \n",
      " 14  reporteddayofweek    281692 non-null  object  \n",
      " 15  reportedhour         281692 non-null  int64   \n",
      " 16  occurrenceyear       281692 non-null  int64   \n",
      " 17  occurrencemonth      281692 non-null  object  \n",
      " 18  occurrenceday        281692 non-null  int64   \n",
      " 19  occurrencedayofyear  281692 non-null  int64   \n",
      " 20  occurrencedayofweek  281692 non-null  object  \n",
      " 21  occurrencehour       281692 non-null  int64   \n",
      " 22  MCI                  281692 non-null  object  \n",
      " 23  Hood_ID              281692 non-null  object  \n",
      " 24  Neighbourhood        281692 non-null  object  \n",
      " 25  Long                 281692 non-null  float64 \n",
      " 26  Lat                  281692 non-null  float64 \n",
      " 27  ObjectId             281692 non-null  int64   \n",
      " 28  geometry             281692 non-null  geometry\n",
      "dtypes: float64(2), geometry(1), int64(12), object(14)\n",
      "memory usage: 62.3+ MB\n"
     ]
    }
   ],
   "source": [
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index_</th>\n",
       "      <th>event_unique_id</th>\n",
       "      <th>Division</th>\n",
       "      <th>occurrencedate</th>\n",
       "      <th>reporteddate</th>\n",
       "      <th>location_type</th>\n",
       "      <th>premises_type</th>\n",
       "      <th>ucr_code</th>\n",
       "      <th>ucr_ext</th>\n",
       "      <th>offence</th>\n",
       "      <th>...</th>\n",
       "      <th>occurrencedayofyear</th>\n",
       "      <th>occurrencedayofweek</th>\n",
       "      <th>occurrencehour</th>\n",
       "      <th>MCI</th>\n",
       "      <th>Hood_ID</th>\n",
       "      <th>Neighbourhood</th>\n",
       "      <th>Long</th>\n",
       "      <th>Lat</th>\n",
       "      <th>ObjectId</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>GO-20141625305</td>\n",
       "      <td>D23</td>\n",
       "      <td>2014-03-02T05:00:00</td>\n",
       "      <td>2014-03-02T05:00:00</td>\n",
       "      <td>Single Home, House (Attach Garage, Cottage, Mo...</td>\n",
       "      <td>House</td>\n",
       "      <td>1430</td>\n",
       "      <td>100</td>\n",
       "      <td>Assault</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>8</td>\n",
       "      <td>Assault</td>\n",
       "      <td>1</td>\n",
       "      <td>West Humber-Clairville</td>\n",
       "      <td>-79.590332</td>\n",
       "      <td>43.734013</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-79.59033 43.73401)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>GO-20141272968</td>\n",
       "      <td>D23</td>\n",
       "      <td>2013-12-24T05:00:00</td>\n",
       "      <td>2014-01-03T05:00:00</td>\n",
       "      <td>Commercial Dwelling Unit (Hotel, Motel, B &amp; B,...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>1610</td>\n",
       "      <td>200</td>\n",
       "      <td>Robbery - Mugging</td>\n",
       "      <td>...</td>\n",
       "      <td>358</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>22</td>\n",
       "      <td>Robbery</td>\n",
       "      <td>1</td>\n",
       "      <td>West Humber-Clairville</td>\n",
       "      <td>-79.600701</td>\n",
       "      <td>43.731834</td>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-79.60070 43.73183)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>GO-20141284361</td>\n",
       "      <td>D23</td>\n",
       "      <td>2013-01-05T05:00:00</td>\n",
       "      <td>2014-01-05T05:00:00</td>\n",
       "      <td>Commercial Dwelling Unit (Hotel, Motel, B &amp; B,...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>1430</td>\n",
       "      <td>100</td>\n",
       "      <td>Assault</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>4</td>\n",
       "      <td>Assault</td>\n",
       "      <td>1</td>\n",
       "      <td>West Humber-Clairville</td>\n",
       "      <td>-79.600794</td>\n",
       "      <td>43.686423</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-79.60079 43.68642)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384</td>\n",
       "      <td>GO-20141292177</td>\n",
       "      <td>D23</td>\n",
       "      <td>2013-12-31T05:00:00</td>\n",
       "      <td>2014-01-06T05:00:00</td>\n",
       "      <td>Other Commercial / Corporate Places (For Profi...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>2120</td>\n",
       "      <td>200</td>\n",
       "      <td>B&amp;E</td>\n",
       "      <td>...</td>\n",
       "      <td>365</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>21</td>\n",
       "      <td>Break and Enter</td>\n",
       "      <td>1</td>\n",
       "      <td>West Humber-Clairville</td>\n",
       "      <td>-79.603876</td>\n",
       "      <td>43.743642</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-79.60388 43.74364)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>438</td>\n",
       "      <td>GO-20141297201</td>\n",
       "      <td>D23</td>\n",
       "      <td>2014-01-03T05:00:00</td>\n",
       "      <td>2014-01-07T05:00:00</td>\n",
       "      <td>Other Commercial / Corporate Places (For Profi...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>2120</td>\n",
       "      <td>200</td>\n",
       "      <td>B&amp;E</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Friday</td>\n",
       "      <td>10</td>\n",
       "      <td>Break and Enter</td>\n",
       "      <td>1</td>\n",
       "      <td>West Humber-Clairville</td>\n",
       "      <td>-79.586443</td>\n",
       "      <td>43.697108</td>\n",
       "      <td>5</td>\n",
       "      <td>POINT (-79.58644 43.69711)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index_ event_unique_id Division       occurrencedate         reporteddate  \\\n",
       "0     110  GO-20141625305      D23  2014-03-02T05:00:00  2014-03-02T05:00:00   \n",
       "1     188  GO-20141272968      D23  2013-12-24T05:00:00  2014-01-03T05:00:00   \n",
       "2     287  GO-20141284361      D23  2013-01-05T05:00:00  2014-01-05T05:00:00   \n",
       "3     384  GO-20141292177      D23  2013-12-31T05:00:00  2014-01-06T05:00:00   \n",
       "4     438  GO-20141297201      D23  2014-01-03T05:00:00  2014-01-07T05:00:00   \n",
       "\n",
       "                                       location_type premises_type  ucr_code  \\\n",
       "0  Single Home, House (Attach Garage, Cottage, Mo...         House      1430   \n",
       "1  Commercial Dwelling Unit (Hotel, Motel, B & B,...    Commercial      1610   \n",
       "2  Commercial Dwelling Unit (Hotel, Motel, B & B,...    Commercial      1430   \n",
       "3  Other Commercial / Corporate Places (For Profi...    Commercial      2120   \n",
       "4  Other Commercial / Corporate Places (For Profi...    Commercial      2120   \n",
       "\n",
       "   ucr_ext            offence  ...  occurrencedayofyear occurrencedayofweek  \\\n",
       "0      100            Assault  ...                   61          Sunday       \n",
       "1      200  Robbery - Mugging  ...                  358          Tuesday      \n",
       "2      100            Assault  ...                    5          Saturday     \n",
       "3      200                B&E  ...                  365          Tuesday      \n",
       "4      200                B&E  ...                    3          Friday       \n",
       "\n",
       "   occurrencehour              MCI Hood_ID           Neighbourhood       Long  \\\n",
       "0               8          Assault       1  West Humber-Clairville -79.590332   \n",
       "1              22          Robbery       1  West Humber-Clairville -79.600701   \n",
       "2               4          Assault       1  West Humber-Clairville -79.600794   \n",
       "3              21  Break and Enter       1  West Humber-Clairville -79.603876   \n",
       "4              10  Break and Enter       1  West Humber-Clairville -79.586443   \n",
       "\n",
       "         Lat  ObjectId                    geometry  \n",
       "0  43.734013         1  POINT (-79.59033 43.73401)  \n",
       "1  43.731834         2  POINT (-79.60070 43.73183)  \n",
       "2  43.686423         3  POINT (-79.60079 43.68642)  \n",
       "3  43.743642         4  POINT (-79.60388 43.74364)  \n",
       "4  43.697108         5  POINT (-79.58644 43.69711)  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281692, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns we don't need\n",
    "cols2drop = ['Index_', 'Division', 'reporteddate', 'location_type', 'ucr_code', 'ucr_ext', 'offence', 'reportedyear', 'reportedmonth', \n",
    "             'reportedday', 'reporteddayofyear', 'reporteddayofweek', 'reportedhour', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "crime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event_unique_id', 'occurrencedate', 'premises_type', 'occurrenceyear', 'occurrencemonth', 'occurrenceday', 'MCI', 'Hood_ID', 'Neighbourhood', 'Long', 'Lat', 'ObjectId', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# 16 columns removed\n",
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    39912\n",
       "2018    37408\n",
       "2017    35484\n",
       "2020    34866\n",
       "2021    33685\n",
       "2016    33596\n",
       "2015    32910\n",
       "2014    32459\n",
       "2013      594\n",
       "2012      182\n",
       "2011      128\n",
       "2010       96\n",
       "0          95\n",
       "2009       71\n",
       "2008       42\n",
       "2007       35\n",
       "2004       26\n",
       "2005       24\n",
       "2000       22\n",
       "2001       20\n",
       "2003       14\n",
       "2002       12\n",
       "2006       11\n",
       "Name: occurrenceyear, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a quick peek at crime by year\n",
    "crime['occurrenceyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280320, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there appears to be a big change in crime data between 2013 and 2014 - a jump by 30 thousand a year!\n",
    "# data from 2014 onwards appears to be similar and represents over 250,000 of the 281,692 entries in this dataset\n",
    "# it is not clear why this is the case - perhaps due to a change in tracking or recording practices\n",
    "# in order to ensure our analysis is not skewed, we will delete all occurences from 2013 or earlier\n",
    "crime = crime[crime['occurrenceyear'] > 2013]\n",
    "\n",
    "# confirm reduced dataframe\n",
    "crime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    39912\n",
       "2018    37408\n",
       "2017    35484\n",
       "2020    34866\n",
       "2021    33685\n",
       "2016    33596\n",
       "2015    32910\n",
       "2014    32459\n",
       "Name: occurrenceyear, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appox 1300 crime events removed\n",
    "# let's take another look at crime by year\n",
    "crime['occurrenceyear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to: crimeData.csv.gz\n",
      "Creating data/raw under /home/jovyan/work/term2/dcsa_grp_project\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# save reduced data locally\n",
    "path = os.path.join('data','raw')\n",
    "fn = 'crimeData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "crime.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload and Clean Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "~/data/raw/crimeData.csv.gz: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ~/data/raw/crimeData.csv.gz: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b96b4d5c6803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reload data from local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/data/raw/crimeData.csv.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcrime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# have a look at data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 254\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ~/data/raw/crimeData.csv.gz: No such file or directory"
     ]
    }
   ],
   "source": [
    "# reload data from local\n",
    "path = '~/data/raw/crimeData.csv.gz'\n",
    "crime = gpd.read_file(path, compression='gzip', low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "'data/rough/crimeData.csv.gz' not recognized as a supported file format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: 'data/rough/crimeData.csv.gz' not recognized as a supported file format.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d062e87e5c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rough'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'crimeData.csv.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcrime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# have a look at data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 254\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: 'data/rough/crimeData.csv.gz' not recognized as a supported file format."
     ]
    }
   ],
   "source": [
    "# reload data from local\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'crimeData.csv.gz'\n",
    "crime = gpd.read_file(os.path.join(path,fn), compression='gzip', low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "crime.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "crime.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no null values - great!\n",
    "# now count nans by columns\n",
    "crime.isna().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nans by rows\n",
    "crime.isna().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no nans - great!\n",
    "# documentation indicates that for locations identified outside the city of Toronto limits or as invalidated locations\n",
    "# the division/neighbourhood designation will be ‘NSA’ to indicate ‘Not Specified Area.’\n",
    "# we will identify and delete these as we are only concerned with occurences with identifiable locations\n",
    "crime.loc[crime['Hood_ID'] == 'NSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that when 'Hood_ID' == NSA, 'Neighbourhood' is also NSA, let's check to see if we get the same results\n",
    "crime.loc[crime['Neighbourhood'] == 'NSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they match! we will delete these\n",
    "crime = crime[crime.Hood_ID != 'NSA']\n",
    "crime = crime[crime.Neighbourhood != 'NSA']\n",
    "\n",
    "# confirm reduced dataframe\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm reduction meets amount in NSA query\n",
    "x = 281692-277071\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "\n",
    "# 1. crime types - 2 fields\n",
    "    # offence and MCI fields - from documentation, we know that offence is a non-standardized open field, whereas MCI is categorized\n",
    "    # we will drop offence and keep MCI\n",
    "    \n",
    "# 2. neighbourhood - 2 fields\n",
    "    # Hood_ID and Neighbourhood fields are duplicates, but should bothe be left in for fleixbility in matching with neighoburhood polygons later\n",
    "    # we will check unique values to ensure both have the same amount and keep both of these \n",
    "    \n",
    "# 3. cateogrical fields - 4 fields\n",
    "    # location_type, premises_type, MCI, and Neighbourhood fields are all categorical data as indicated in the documentation\n",
    "    # location_type is too detailed for our purposes\n",
    "    # we will delete location_type and convert the rest to categories - NOTE GPKG WOULD NOT ALLOW CATEGORIES\n",
    "    \n",
    "# 4. ids - 2 fields\n",
    "    # event_unique_id and ObjectId fields seem to both be unique values, but they are different data types\n",
    "    # documentation suggests that occurences with multiple types of crimes will show up as multiple entries\n",
    "    # therefore, one of these fields will have multiple entries and the other will have unique values - we will need to confirm differences, address them, and possibly convert to int\n",
    "\n",
    "# 5. dates - 7 fields\n",
    "    # occurrencedate, occurrenceyear, occurrencemonth, occurrenceday, occurrencedayofyear, occurrencedayofweek, and occurrencehour fields\n",
    "    # occurencedate is generalized and is sufficient for our purposes, we will delete the rest\n",
    "    \n",
    "# 6. location - 3 fields\n",
    "    # we will keep geometry\n",
    "    # we will check lat and long to see if there are outliers to remove, then we will delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - drop offence field\n",
    "cols2drop2 = ['location_type','offence']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop2:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - count unique values of neighbourhood fields\n",
    "count1 = crime.Hood_ID.nunique()\n",
    "count2 = crime.Neighbourhood.nunique()\n",
    "print(count1)\n",
    "print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they match and we will keep both\n",
    "\n",
    "# NOTE gpkg will not accept categories so the rest of this cell has been hashed out\n",
    "\n",
    "# 3 - convert location_type, premises_type, MCI, and neighbourhood fields to cateogrical data\n",
    "#for c in ['premises_type', 'MCI', 'Neighbourhood']:\n",
    "#    crime[c] = crime[c].astype('category')\n",
    "    \n",
    "# confirm conversion to categories\n",
    "#crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also convert Hood_ID to int\n",
    "for c in ['Hood_ID']:\n",
    "    crime[c] = crime[c].astype('int')\n",
    "\n",
    "# confirm conversion to int\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - we have 277071 rows, let's check count of unique values of id fields compared to rows\n",
    "count3 = crime.event_unique_id.nunique()\n",
    "count4 = crime.ObjectId.nunique()\n",
    "print(count3)\n",
    "print(count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the event_unique_id field has duplicate values, whereas the ObjectID field has a unique value for every row\n",
    "# as per documentation one event can have mutliple crime types - for example, a single event could have both an assault and a theft\n",
    "# for our purposes, we want to ensure we count each crime type seperately\n",
    "# we will delete the event_unique_id field and keep the ObjectId field\n",
    "cols2drop3 = ['event_unique_id']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop3:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - check dates fields\n",
    "dates = crime[['occurrencedate', 'occurrenceyear', 'occurrencemonth', 'occurrenceday', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']]\n",
    "dates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our purposes occurrencedate is all that is required and the rest can be dropped\n",
    "cols2drop4 = ['occurrenceyear', 'occurrencemonth', 'occurrenceday', 'occurrencedayofyear', 'occurrencedayofweek', 'occurrencehour']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop4:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['occurrencedate'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - we will check lat and long fields for outliers\n",
    "location = crime[['Long', 'Lat']]\n",
    "location.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min in the lat field is showing as 0 which is definitely not in Toronto\n",
    "\n",
    "# let's check how many values are equal to 0\n",
    "print((crime['Lat'] == 0).sum())\n",
    "\n",
    "# vs. all values\n",
    "print((crime['Lat']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete these values\n",
    "crime = crime[crime.Lat != 0]\n",
    "\n",
    "# check values again\n",
    "location2 = crime[['Long', 'Lat']]\n",
    "location2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that fixed the problem\n",
    "# we can now delete Long and Lat columns\n",
    "cols2drop5 = ['Long', 'Lat']\n",
    "\n",
    "for col in crime.columns:\n",
    "    if col in cols2drop5:\n",
    "        crime.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm removal - makes copying text for later steps easier\n",
    "print(list(crime.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have another look at the data\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 278 rows have been removed\n",
    "# another look\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are completing an analysis of the impact of COVID-19 lockdowns on crime, we will add an additonal column for lockdown information\n",
    "# from research we know that Toronto had three lockdowns as follows:\n",
    "\n",
    "lockdown = {'ONE':['2020-03-23','2020-07-31'],\n",
    "            'TWO':['2020-11-23','2021-03-08'],\n",
    "            'THREE':['2021-04-08','2021-06-02']}\n",
    "\n",
    "crime['occurrencedate'] = pd.to_datetime(crime.occurrencedate.values, infer_datetime_format=True)\n",
    "\n",
    "for k, (s,e) in lockdown.items():\n",
    "    crime.loc[crime['occurrencedate'].between(s,e), 'lockdownNum'] = k\n",
    "\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in lockdownNum field with NONE (no lockdown)\n",
    "crime['lockdownNum'] = crime['lockdownNum'].fillna('NONE')\n",
    "\n",
    "# check revised number of occurrences by lockdownNum\n",
    "crime.lockdownNum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE gpkg will not accept categories so this cell has been hashed out\n",
    "\n",
    "# convert lockdownNum field to categorical data\n",
    "#for c in ['lockdownNum']:\n",
    "#    crime[c] = crime[c].astype('category')\n",
    "\n",
    "# confirm changes\n",
    "#crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary lockdown column based on lockdownNum column where 1 = in lockdown, and 0 = not in lockdown\n",
    "crime['lockdownBinary'] = ['1' if x == 'ONE' else '1' if x == 'TWO' else '1' if x == 'THREE' else '0' for x in crime['lockdownNum']]\n",
    "\n",
    "#confirm new column\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lockdownBinary to int\n",
    "for c in ['lockdownBinary']:\n",
    "    crime[c] = crime[c].astype('int')\n",
    "    \n",
    "# confirm conversion to int\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### crime.sort_values(by=['occurrenceyear', 'occurrencemonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random check of data frame\n",
    "crime.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at data\n",
    "crime.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save crime as clean data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'crimeData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "crime.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save crime as geodataframe\n",
    "\n",
    "# Set save location\n",
    "path = os.path.join('data','geo')\n",
    "fn = 'crimeData.gpkg'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "\n",
    "crime.to_file(os.path.join(path,fn), index=False, driver='GPKG')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at the Covid case data\n",
    "# open covid-19 case data into a pandas dataframe\n",
    "\n",
    "url = 'https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/64b54586-6180-4485-83eb-81e8fae3b8fe/resource/fff4ee65-3527-43be-9a8a-cb9401377dbc/download/COVID19%20cases.csv'\n",
    "covidcases = pd.read_csv(url, low_memory=False)\n",
    "\n",
    "# confirm load\n",
    "print(f\"Data frame is {covidcases.shape[0]:,} rows x {covidcases.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "covidcases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(covidcases.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data\n",
    "covidcases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need\n",
    "cols2drop6 = ['Outbreak Associated', 'Age Group', 'Source of Infection', 'Client Gender', 'Outcome', 'Currently Hospitalized', \n",
    "              'Currently in ICU', 'Currently Intubated', 'Ever Hospitalized', 'Ever in ICU', 'Ever Intubated']\n",
    "for col in df.columns:\n",
    "    if col in cols2drop6:\n",
    "        df.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rough covid case data\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'covidData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "covidcases.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload rough covid case data\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'covidData.csv.gz'\n",
    "df = pd.read_csv(os.path.join(path,fn), compression='gzip', low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(df.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "df.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "df.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many null values\n",
    "# let's look at remaining columns in more detail to see if we can drop more\n",
    "df.head()[['_id', 'Assigned_ID', 'Neighbourhood Name', 'FSA', 'Classification', 'Episode Date', 'Reported Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "# 1. ids - 2 fields\n",
    "    # the _id and Assigned_ID fields seem to be identical and have the same amount\n",
    "    # however, documentation suggests Assigned_ID is from Toronto Public Health, not the database, and cases can disappear\n",
    "    # based on this, we will delete Assigned_ID\n",
    "\n",
    "# 2. Locations - 2 fields\n",
    "    # the Neighbourhood Name and Forward Sortation Area (FSA) fields both can be used to geolocate the cases\n",
    "    # documentation suggests that the FSA field and Census Tracts (CTs) were used to determine the Neighbourhood Name\n",
    "    # documentation also mentions that neighbourhood information is missing for cases with missing postalcodes\n",
    "    # for our purposes aggregate numbers are okay and we don't need to geolocate so we will drop both of these fields \n",
    "\n",
    "# 3. Status - 1 field\n",
    "    # According to the documentation, the Classification field classifies cases as either confirmed or probable\n",
    "    # as with above, for our purposes, aggregate numbers are okay, we will delete probable cases and keep confirmed\n",
    "    \n",
    "# 4. dates - 2 fields\n",
    "    # we do not need both of these dates, so we will keep the reported date as an official record and drop the episode date\n",
    "    # change reported date to reported_date for ease\n",
    "    \n",
    "# drop columns we don't need\n",
    "cols2drop7 = ['Assigned_ID', 'Neighbourhood Name', 'FSA', 'Episode Date']\n",
    "for col in df.columns:\n",
    "    if col in cols2drop7:\n",
    "        df.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names again - makes copying text for later steps easier\n",
    "print(list(df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by columns to identify if there are any problems\n",
    "df.isnull().sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nulls by rows\n",
    "df.isnull().sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more nulls - perfect!\n",
    "# check classification counts \n",
    "df.Classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove PROBABLE cases, keep CONFIRMED cases\n",
    "df = df[df.Classification != 'PROBABLE']\n",
    "\n",
    "# confirm reduced dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts\n",
    "df.Classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename reported date field\n",
    "df.rename(columns = {'Reported Date':'ReportedDate'}, inplace = True)\n",
    "\n",
    "# confirm renamed column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last thing we want to do is to get a summary count of all confirmed cases per day\n",
    "casesperday = df.groupby(['ReportedDate']).size().reset_index(name='CasesPerDay')\n",
    "casesperday = casesperday.sort_values(by=['CasesPerDay'], ascending=False)\n",
    "casesperday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned COVID Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned covid case data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'covidData.csv.gz'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "casesperday.to_csv(os.path.join(path,fn), index=False, compression=\"gzip\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Key Dates Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at key dates\n",
    "# load key dates data into a pandas dataframe\n",
    "\n",
    "path = os.path.join('data','rough')\n",
    "fn = 'keyDates.csv'\n",
    "kd = pd.read_csv(os.path.join(path,fn), low_memory=False)\n",
    "\n",
    "# have a look at data\n",
    "kd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "kd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data review\n",
    "\n",
    "# 1. dates - 3 fields\n",
    "    # Date field - refers to date of event (see keydates) - will leave as object\n",
    "    # LockdownDates - is binary - refers to the dates the lockdowns began and ended - will be used for visualization - will leave as int\n",
    "    # KeyDates - is binary - refers to key dates for headlines in the visualization - will leave as int\n",
    "    \n",
    "# 2. Event - 1 field\n",
    "    # can be used as headlines or scrolling tickers for the visualization\n",
    "    \n",
    "# 3. cateogrical fields - 1 fields\n",
    "    # Lockdown field - represents which lockdown\n",
    "    # we will convert these to categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Key Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - convert Lockdown field to cateogrical data\n",
    "for c in ['Lockdown']:\n",
    "    kd[c] = kd[c].astype('category')\n",
    "    \n",
    "# confirm conversion to categories\n",
    "kd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Key Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned key dates data\n",
    "path = os.path.join('data','clean')\n",
    "fn = 'keyDates.csv'\n",
    "print(f\"Writing to: {fn}\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "kd.to_csv(os.path.join(path,fn), index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbourhood Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at the neighbourhood boundaries\n",
    "# read in json file\n",
    "tor_nbs = gpd.read_file('https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/4def3f65-2a65-4a4f-83c4-b2a4aed72d46/resource/9ce32bd1-91ac-4422-925a-bdc256702756/download/Neighbourhoods%20-%20historical%20140.geojson')\n",
    "\n",
    "# confirm load\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "tor_nbs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names - makes copying text for later steps easier\n",
    "print(list(tor_nbs.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need - we only need to be able to match neighbourhood geography to Hood_ID and Neighbourhood columns within the crimeData table\n",
    "cols2drop8 = ['AREA_ID', 'AREA_ATTR_ID', 'PARENT_AREA_ID', 'AREA_DESC', 'CLASSIFICATION', 'CLASSIFICATION_CODE', 'OBJECTID']\n",
    "for col in tor_nbs.columns:\n",
    "    if col in cols2drop8:\n",
    "        tor_nbs.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# confirm reduced dataframe\n",
    "tor_nbs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data review\n",
    "\n",
    "# 1 - area name fields\n",
    "# we will rename this column to Neighbourhood to match our data\n",
    "\n",
    "# 2 - area code fields\n",
    "# we need to confirm which of short_code or long_code is more accurate, then delete the other\n",
    "# we will then rename the column to Hood_ID to match our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - rename area name column to Neighbourhood to match our data\n",
    "tor_nbs.rename(columns={'AREA_NAME': 'Neighbourhood'}, inplace=True)\n",
    "\n",
    "# check to confirm\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - area code fields\n",
    "# we need to confirm which of short_code or long_code is more accurate, then delete the other\n",
    "# we will then rename the column to Hood_ID to match our data\n",
    "count5 = tor_nbs.AREA_SHORT_CODE.nunique()\n",
    "count6 = tor_nbs.AREA_LONG_CODE.nunique()\n",
    "print(count5)\n",
    "print(count6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they seem to be identical, let's do one more check\n",
    "tor_nbs.AREA_SHORT_CODE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and...\n",
    "tor_nbs.AREA_LONG_CODE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmed, they are identifcal, we will delete AREA_LONG_CODE...\n",
    "cols2drop9 = ['AREA_LONG_CODE']\n",
    "for col in tor_nbs.columns:\n",
    "    if col in cols2drop9:\n",
    "        tor_nbs.drop(col, inplace=True, axis=1)\n",
    "        \n",
    "# .. and change the name of AREA_SHORT_CODE to Hood_ID\n",
    "tor_nbs.rename(columns={'AREA_SHORT_CODE': 'Hood_ID'}, inplace=True)\n",
    "        \n",
    "# confirm changes\n",
    "tor_nbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check plot \n",
    "print(tor_nbs.geometry.crs)\n",
    "print(tor_nbs.total_bounds)\n",
    "ax = tor_nbs.plot(figsize=(18,14), \n",
    "                  edgecolor='red', \n",
    "                  facecolor='none', \n",
    "                  linewidth=1, \n",
    "                  alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see how things look\n",
    "\n",
    "# plot crime\n",
    "print(crime.geometry.crs)\n",
    "print(crime.total_bounds)\n",
    "crime.plot(figsize=(18,14), marker='*', color='green', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add toronto neighbourhoods\n",
    "base = tor_nbs.plot(figsize=(18,14), color='white', edgecolor='black')\n",
    "\n",
    "# add crime locations\n",
    "crime.plot(ax=base, figsize=(18,14), marker='o', color='red', markersize=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks good!\n",
    "# NOTE - it appears that some dots along the edges are outside toronto\n",
    "    # HOWEVER - documentation notes crime locations moved to closest intersection to maintain annonimity\n",
    "    # AS A RESULT - All included points have been verified as within an identified Toronto Neighbourhood, but may not appear so visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Neighbourhood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tor_nbs as a geodataframe\n",
    "\n",
    "# Set save location\n",
    "path = os.path.join('data','geo')\n",
    "fn = 'tor_nbs.gpkg'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"Creating {path} under {os.getcwd()}\")\n",
    "    os.makedirs(path)\n",
    "    \n",
    "print(f\"Using '{fn}' as basis for saving data...\")\n",
    "\n",
    "tor_nbs.to_file(os.path.join(path,fn), driver='GPKG')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
